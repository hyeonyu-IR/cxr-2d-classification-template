{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5290c3f-9c54-47d2-9a73-dc8d3ce515f9",
   "metadata": {},
   "source": [
    "### 1. Imports + path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d72c20-9da1-4e3d-9a37-3344fe3c8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[0]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"Project root:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16cae3c-854a-4ddf-8d7f-01cd15c1a976",
   "metadata": {},
   "source": [
    "### 2. Load config and rebuild data/loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9339a96-2277-4614-a58b-4e0303d0ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.config import Config, seed_everything\n",
    "from src.utils import load_json\n",
    "\n",
    "# Set this to your actual run directory from 01_train\n",
    "RUN_DIR = r\"outputs\\runs\\medimg_baseline_cls_YYYYMMDD_HHMMSS\"  # <- replace\n",
    "cfg_dict = load_json(str(Path(RUN_DIR) / \"config.json\"))\n",
    "cfg = Config(**cfg_dict)\n",
    "\n",
    "seed_everything(cfg.seed, cfg.deterministic)\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9897f-93f0-4078-994f-6be82a7766dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import build_datasets, build_loaders\n",
    "\n",
    "ds = build_datasets(\n",
    "    root_dir=cfg.data_root,\n",
    "    class_names=cfg.class_names,\n",
    "    image_size=cfg.image_size,\n",
    "    rebuild_balanced_val=cfg.rebuild_balanced_val,\n",
    "    val_n_per_class=cfg.val_n_per_class,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "loaders = build_loaders(\n",
    "    train_ds=ds[\"train_ds\"],\n",
    "    val_ds=ds[\"val_ds\"],\n",
    "    test_ds=ds[\"test_ds\"],\n",
    "    train_items=ds[\"train_items\"],\n",
    "    class_names=cfg.class_names,\n",
    "    batch_size=cfg.batch_size,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    use_weighted_sampler=cfg.use_weighted_sampler,\n",
    ")\n",
    "\n",
    "train_loader = loaders[\"train_loader\"]\n",
    "val_loader   = loaders[\"val_loader\"]\n",
    "test_loader  = loaders[\"test_loader\"]\n",
    "\n",
    "train_items = ds[\"train_items\"]\n",
    "val_items   = ds[\"val_items\"]\n",
    "test_items  = ds[\"test_items\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10c33d-e0a2-4733-b2f8-230da076f3e7",
   "metadata": {},
   "source": [
    "### 3. Load model + best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdd836-9dac-44b3-9ec3-dc692de210a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.models import build_model, get_gradcam_target_layer\n",
    "\n",
    "arch = \"resnet18\"  # must match what you trained with; later you can store in config if desired\n",
    "model = build_model(arch=arch, num_classes=len(cfg.class_names), pretrained=False, device=cfg.device)\n",
    "\n",
    "best_ckpt_path = str(Path(RUN_DIR) / \"best.pt\")\n",
    "state = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model.to(cfg.device)\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "target_layer = get_gradcam_target_layer(model, arch)\n",
    "\n",
    "print(\"Loaded checkpoint:\", best_ckpt_path)\n",
    "print(\"Grad-CAM target layer:\", target_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c969168-79e6-4bfc-8bf6-5fe096e5eca7",
   "metadata": {},
   "source": [
    "### 4. Evaluate VAL/TEST + thresholding + hard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21a734-7475-4c71-a101-9d837011faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "from src.eval import (\n",
    "    eval_split,\n",
    "    metrics_at_threshold,\n",
    "    pick_threshold_max_f1,\n",
    "    pick_threshold_target_sens,\n",
    "    find_errors_binary,\n",
    ")\n",
    "\n",
    "val_out = eval_split(\n",
    "    model=model, loader=val_loader, device=cfg.device, criterion=criterion,\n",
    "    class_names=cfg.class_names, pos_class_name=cfg.pos_class_name\n",
    ")\n",
    "test_out = eval_split(\n",
    "    model=model, loader=test_loader, device=cfg.device, criterion=criterion,\n",
    "    class_names=cfg.class_names, pos_class_name=cfg.pos_class_name\n",
    ")\n",
    "\n",
    "print(\"=== Checkpoint performance ===\")\n",
    "print(f\"VAL  | loss={val_out['loss']:.4f} acc={val_out['acc']:.4f} AP={val_out.get('ap', float('nan')):.4f}\")\n",
    "print(f\"TEST | loss={test_out['loss']:.4f} acc={test_out['acc']:.4f} AP={test_out.get('ap', float('nan')):.4f}\")\n",
    "\n",
    "# Thresholds from VAL\n",
    "thr_f1, best_f1 = pick_threshold_max_f1(val_out[\"y_true\"], val_out[\"y_score\"])\n",
    "m_val_f1 = metrics_at_threshold(val_out[\"y_true\"], val_out[\"y_score\"], thr_f1)\n",
    "\n",
    "TARGET_SENS = 0.95\n",
    "thr_sens, m_val_sens = pick_threshold_target_sens(val_out[\"y_true\"], val_out[\"y_score\"], target_sens=TARGET_SENS)\n",
    "\n",
    "print(\"\\n=== Thresholds selected on VAL ===\")\n",
    "print(f\"Max-F1 threshold: {thr_f1:.4f} (F1={best_f1:.3f})\")\n",
    "print(\"VAL metrics @ thr_f1:\", {k: v for k, v in m_val_f1.items() if k != \"confusion_matrix\"})\n",
    "\n",
    "print(f\"\\nTarget-sensitivity threshold: {thr_sens:.4f} (target sens >= {TARGET_SENS})\")\n",
    "print(\"VAL metrics @ thr_sens:\", {k: v for k, v in m_val_sens.items() if k != \"confusion_matrix\"})\n",
    "\n",
    "# Apply thresholds to TEST\n",
    "m_test_f1 = metrics_at_threshold(test_out[\"y_true\"], test_out[\"y_score\"], thr_f1)\n",
    "m_test_sens = metrics_at_threshold(test_out[\"y_true\"], test_out[\"y_score\"], thr_sens)\n",
    "\n",
    "print(\"\\n=== TEST metrics using thresholds chosen on VAL ===\")\n",
    "print(f\"\\n[TEST @ Max-F1 thr={thr_f1:.4f}]\")\n",
    "print({k: v for k, v in m_test_f1.items() if k != \"confusion_matrix\"})\n",
    "\n",
    "print(f\"\\n[TEST @ TargetSens thr={thr_sens:.4f} (target sens {TARGET_SENS})]\")\n",
    "print({k: v for k, v in m_test_sens.items() if k != \"confusion_matrix\"})\n",
    "\n",
    "print(\"\\nConfusion matrix format: [[TN, FP],[FN, TP]]\")\n",
    "print(\"\\nTEST confusion @ thr_f1:\\n\", m_test_f1[\"confusion_matrix\"])\n",
    "print(\"\\nTEST confusion @ thr_sens:\\n\", m_test_sens[\"confusion_matrix\"])\n",
    "\n",
    "# Hard errors (use Max-F1 threshold by default)\n",
    "errs = find_errors_binary(\n",
    "    items=test_items,\n",
    "    y_score=test_out[\"y_score\"],\n",
    "    pos_idx=test_out[\"pos_idx\"],\n",
    "    thr=thr_f1,\n",
    "    topk=10,\n",
    ")\n",
    "fp_idx = errs[\"fp_idx\"]\n",
    "fn_idx = errs[\"fn_idx\"]\n",
    "\n",
    "print(\"\\n=== Hard errors on TEST (Max-F1 threshold) ===\")\n",
    "print(\"Top false positives:\")\n",
    "for i in fp_idx:\n",
    "    print(f\"  score={test_out['y_score'][i]:.3f} | path={test_items[i]['image']}\")\n",
    "print(\"\\nTop false negatives:\")\n",
    "for i in fn_idx:\n",
    "    print(f\"  score={test_out['y_score'][i]:.3f} | path={test_items[i]['image']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7a8de-524c-4bf6-8fc8-2e2164584b3c",
   "metadata": {},
   "source": [
    "### 5. Grad-CAM: display a few and save batches to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67b248-33bb-46ea-a092-f9ef17f4ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interpret import GradCAM, show_gradcam, save_gradcam_batch, infer_true_label_from_path\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "# Show a couple interactively\n",
    "print(\"=== Show Grad-CAM: first 2 FPs ===\")\n",
    "for i in fp_idx[:2]:\n",
    "    p = test_items[i][\"image\"]\n",
    "    show_gradcam(\n",
    "        model=model,\n",
    "        gradcam=gradcam,\n",
    "        path=p,\n",
    "        class_names=cfg.class_names,\n",
    "        pos_class_name=cfg.pos_class_name,\n",
    "        device=cfg.device,\n",
    "        image_size=cfg.image_size,\n",
    "        true_label_name=infer_true_label_from_path(p),\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Show Grad-CAM: first 1 FN ===\")\n",
    "for i in fn_idx[:1]:\n",
    "    p = test_items[i][\"image\"]\n",
    "    show_gradcam(\n",
    "        model=model,\n",
    "        gradcam=gradcam,\n",
    "        path=p,\n",
    "        class_names=cfg.class_names,\n",
    "        pos_class_name=cfg.pos_class_name,\n",
    "        device=cfg.device,\n",
    "        image_size=cfg.image_size,\n",
    "        true_label_name=infer_true_label_from_path(p),\n",
    "    )\n",
    "\n",
    "# Save panels to disk under the run directory\n",
    "out_dir = Path(RUN_DIR) / \"gradcam\"\n",
    "fp_paths = [test_items[i][\"image\"] for i in fp_idx]\n",
    "fn_paths = [test_items[i][\"image\"] for i in fn_idx]\n",
    "\n",
    "records_fp = save_gradcam_batch(\n",
    "    model=model,\n",
    "    gradcam=gradcam,\n",
    "    paths=fp_paths,\n",
    "    out_dir=str(out_dir / \"FP\"),\n",
    "    class_names=cfg.class_names,\n",
    "    device=cfg.device,\n",
    "    image_size=cfg.image_size,\n",
    "    alpha=cfg.gradcam_alpha,\n",
    ")\n",
    "\n",
    "records_fn = save_gradcam_batch(\n",
    "    model=model,\n",
    "    gradcam=gradcam,\n",
    "    paths=fn_paths,\n",
    "    out_dir=str(out_dir / \"FN\"),\n",
    "    class_names=cfg.class_names,\n",
    "    device=cfg.device,\n",
    "    image_size=cfg.image_size,\n",
    "    alpha=cfg.gradcam_alpha,\n",
    ")\n",
    "\n",
    "print(\"Saved FP panels:\", len(records_fp), \"to\", out_dir / \"FP\")\n",
    "print(\"Saved FN panels:\", len(records_fn), \"to\", out_dir / \"FN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f29b5-7e8e-4dcb-825b-fc66e8d5c38c",
   "metadata": {},
   "source": [
    "### 6. (Optional) Save evaluation summary JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e89b94-1a6a-4498-b83a-a59b087c0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_json\n",
    "\n",
    "summary = {\n",
    "    \"val\": {\n",
    "        \"loss\": float(val_out[\"loss\"]),\n",
    "        \"acc\": float(val_out[\"acc\"]),\n",
    "        \"ap\": float(val_out.get(\"ap\", float(\"nan\"))),\n",
    "        \"thr_f1\": float(thr_f1),\n",
    "        \"thr_sens\": float(thr_sens),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"loss\": float(test_out[\"loss\"]),\n",
    "        \"acc\": float(test_out[\"acc\"]),\n",
    "        \"ap\": float(test_out.get(\"ap\", float(\"nan\"))),\n",
    "        \"metrics_at_thr_f1\": {k: v for k, v in m_test_f1.items() if k != \"confusion_matrix\"},\n",
    "        \"metrics_at_thr_sens\": {k: v for k, v in m_test_sens.items() if k != \"confusion_matrix\"},\n",
    "        \"cm_thr_f1\": m_test_f1[\"confusion_matrix\"].tolist(),\n",
    "        \"cm_thr_sens\": m_test_sens[\"confusion_matrix\"].tolist(),\n",
    "    },\n",
    "}\n",
    "\n",
    "save_json(summary, str(Path(RUN_DIR) / \"eval_summary.json\"))\n",
    "print(\"Saved:\", Path(RUN_DIR) / \"eval_summary.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
